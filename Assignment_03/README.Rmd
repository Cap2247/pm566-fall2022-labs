---
title: "Assignment 3"
author: "CP"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(httr)
library(tidyverse)
library(stringr)
library(xml2)
```

# 1. API
```{r}
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2%20trial%20vaccine")
  
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]")
counts <- as.character(counts)
#stringr::str_extract(counts, "[REGEX FOR NUMBERS WITH COMMAS/DOTS]")
```

 There were 4009 paper for this search  
```{r}
query_ids <- GET(
  url      = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query    = list(
    db     = "pubmed",
    term   = "sars-cov-2 trial vaccine",
    retmax = 250
  )
)
```

```{r}
ids <- httr::content(query_ids)
```

```{r}
ids <- as.character(ids)
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
ids <- stringr::str_remove_all(ids, "<?Id>")
ids <- stringr::str_remove_all(ids, "</")
head(ids)
```

```{r}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db      = "pubmed", 
    Id      = paste(ids, collapse = ","),
    retmax  = 250, 
    rettype = "abstract" 
    )
)
publications <- httr::content(publications)

```

```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```

```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts[[1]]
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>") 
abstracts[[1]]
abstracts <- str_replace_all(abstracts, "[[:space:]]+"," ")
abstracts[[1]]
```

```{r}
journal_title <- str_extract(pub_char_list, "<Title>[[:print:][:space:]]+</Title>")
journal_title[[1]]
journal_title <- str_remove_all(journal_title, "</?[[:alnum:]- =\"]+>") 
journal_title[[1]]
journal_title <- str_replace_all(journal_title, "[[:space:]]+"," ")
journal_title[[1]]
```

```{r}
pubdate <- str_extract(pub_char_list, "<PubDate>[[:print:][:space:]]+</PubDate>")
pubdate[[1]]
pubdate<- str_remove_all(pubdate, "</?[[:alnum:]- =\"]+>") 
pubdate[[1]]
pubdate <- str_replace_all(pubdate, "[[:space:]]+"," ")
pubdate[[1]]
```

```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles[[1]]
titles <- str_remove_all(titles, "</?[[:alnum:]- =\"]+>")
titles[[1]]
```

```{r}
database <- data.frame(
  PubMedId = ids,
  Journal  = journal_title,
  Abstract = abstracts,
  ArtTitle = titles,
  Pubdate  = pubdate
)
knitr::kable(database[1:8,], caption = "Sars-Cov-2 Vaccine Trial Articles")
```


# 2. Text Mining 
```{r}
library(tidytext)
library(ggplot2)
library(dplyr)
library(forcats)
```


```{r}
if (!file.exists("pubmed.csv")){
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv", "pubmed.csv", method="libcurl", timeout = 60)
}

pubmed <- read.csv("pubmed.csv")
pubmed <- as.tibble(pubmed)
str(pubmed)
```

## 1. Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

```{r}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  count(token, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```
The first seven words are stop words that will have to be removed to see the next most utilized words


```{r}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```
```{r}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE) %>%
  top_n(5, n) %>%
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col() + labs(title = " Top 5 tokens in Pubmed Abstracts")
```
Removing the stop words that are common articles in manuscripts reveals that covid, and  19 are the most utilized words.  

## 2. Tokenize the abstracts into bigrams. Find the 10 most common bigram and visualize them with ggplot2.

```{r}
pubmed %>%
  unnest_ngrams(bigram, abstract, n=2) %>%
  
  count(bigram, sort = TRUE) %>%
  top_n(10, n) %>%
  ggplot(aes(n, fct_reorder(bigram, n))) +
  geom_col()
```

When we create a bigram we see covid 19 is infact the most frequently used word. 

```{r}
pubmed %>%
  unnest_ngrams(bigram, abstract, n=2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(bigram, n))) +
  geom_col()

```

## 3. Calculate the TF-IDF value for each word-search term combination. (here you want the search term to be the “document”) What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

```{r}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  count(term, token) %>%
  bind_tf_idf(term, token, n) %>%
  arrange(desc(tf_idf))
```

```{r}
pubmed %>%
  unnest_tokens(token, abstract) %>%
  count(token, term) %>%
  bind_tf_idf(token, term, n) %>%
  arrange(desc(tf_idf))
```
Covid and prostate have the highest tf-idf value. This indicates that these words are more relevant in these abstracts. This differs from the findings in question 1 because 19 was a frequently used words as well as patient, however this proves that they were not as significant. 